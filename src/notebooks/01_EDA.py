# -*- coding: utf-8 -*-
"""01_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JfvoQAnZZW7sYJiSPekbx0RGjl8x4N0H
"""

# -*- coding: utf-8 -*-
"""
EDA + Feature Engineering Rationale + Visualization Pipeline

This module produces a step-by-step EDA and justification bundle that matches the
feature engineering and preprocessing strategy you described (Yeoâ€“Johnson "windowizing",
categorical encoding strategy, and SMOTE). It generates:
  - Visualizations for numeric and categorical features
  - Pre/post transformation ("windowizing") comparisons
  - Class imbalance diagnostics with SMOTE before/after PCA scatter plots
  - Rationale tables explaining why each feature exists or was engineered
  - Basic feature relevance via Mutual Information and RandomForest importance
  - A concise Markdown report summarizing each step

USAGE
-----
1) Place this file in the same environment as your DataPreprocessor class (the one you provided).
2) Adjust `train_paths` in the __main__ section to match your file names.
3) Run the script. It will produce figures and a markdown report in `eda_output/`.
4) Optionally run your `DataPreprocessor.preprocess_and_save()` for train-ready data dumps.

DEPENDENCIES
------------
- numpy, pandas, matplotlib, seaborn (optional, with fallback), scipy, scikit-learn
- imbalanced-learn (optional: SMOTE demo will be skipped if not available)
- statsmodels (optional: VIF calculation will be skipped if not available)

NOTES
-----
- "Windowizing" here refers to Yeoâ€“Johnson power transformation applied to skewed numeric features.
- SMOTE should be applied to the training fold ONLY to avoid data leakage. The demo here visualizes
  the effect but does not alter your saved datasets unless you call your own preprocessor.
"""

# ---------------------------------------------------------------------------
# If your DataPreprocessor is in another module, import it here:
# from your_module import DataPreprocessor
#
# In this script I assume you already have the provided DataPreprocessor class
# available in the environment
# ---------------------------------------------------------------------------

import os
import gc
import time
import pickle
import warnings
from typing import List, Tuple, Dict, Optional, Union
from pathlib import Path
from functools import lru_cache
import logging

warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Try to import seaborn for nicer plots; fall back to matplotlib only if missing.
try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False

from scipy.stats import chi2_contingency
from sklearn.preprocessing import StandardScaler, PowerTransformer, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.feature_selection import mutual_info_classif
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

# Optional: statsmodels for VIF
try:
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    import statsmodels.api as sm
    HAS_SM = True
except ImportError:
    HAS_SM = False

# Optional: imbalanced-learn for SMOTE
try:
    from imblearn.over_sampling import SMOTE
    HAS_IMBLEARN = True
except ImportError:
    HAS_IMBLEARN = False

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Try to import tqdm for progress bars
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # Create a dummy tqdm class if not available
    class tqdm:
        def __init__(self, *args, **kwargs):
            pass
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
        def update(self, n=1):
            pass
        def set_description(self, desc):
            pass

# =============================================================================
# Progress Tracking and Timing Utilities
# =============================================================================

class ProgressTracker:
    """Track progress and estimate time for long-running operations."""

    def __init__(self, total_steps: int, description: str = "Processing"):
        self.total_steps = total_steps
        self.current_step = 0
        self.start_time = time.time()
        self.step_times = []
        self.description = description

    def start_step(self, step_name: str):
        """Start timing a step."""
        self.step_start_time = time.time()
        self.current_step += 1
        elapsed = time.time() - self.start_time

        if self.current_step > 1:
            avg_time_per_step = elapsed / (self.current_step - 1)
            remaining_steps = self.total_steps - self.current_step + 1
            estimated_remaining = avg_time_per_step * remaining_steps

            logger.info(f"ðŸ”„ [{self.current_step}/{self.total_steps}] {step_name}")
            logger.info(f"   â±ï¸  Elapsed: {self._format_time(elapsed)} | "
                       f"Estimated remaining: {self._format_time(estimated_remaining)}")
        else:
            logger.info(f"ðŸš€ [{self.current_step}/{self.total_steps}] {step_name}")

    def end_step(self):
        """End timing a step."""
        step_time = time.time() - self.step_start_time
        self.step_times.append(step_time)

    def finish(self):
        """Finish tracking and show summary."""
        total_time = time.time() - self.start_time
        logger.info(f"âœ… {self.description} completed in {self._format_time(total_time)}")

    def _format_time(self, seconds: float) -> str:
        """Format time in human-readable format."""
        if seconds < 60:
            return f"{seconds:.1f}s"
        elif seconds < 3600:
            return f"{seconds/60:.1f}m"
        else:
            return f"{seconds/3600:.1f}h"


def time_function(func):
    """Decorator to time function execution."""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        logger.info(f"â±ï¸  {func.__name__} completed in {end_time - start_time:.2f}s")
        return result
    return wrapper

class DataPreprocessor:
    """Handles all data preprocessing including windowizing with optimized performance."""

    def __init__(self, random_state: int = 42):
        """Initialize the preprocessor with configurable random state."""
        self.label_encoders = {}
        self.scaler = StandardScaler()
        self.power_transformers = {}
        self.alternative_features = []
        self.traditional_features = []
        self.all_features = []
        self.random_state = random_state

    def load_and_merge_data(self, data_paths: Dict[str, str]) -> Tuple[pd.DataFrame, Optional[np.ndarray]]:
        """Load and merge all datasets with optimized memory usage."""
        logger.info("Loading datasets...")

        # Load main application data
        app_data = pd.read_csv(data_paths['application'])
        logger.info(f"Application data shape: {app_data.shape}")

        # Store target if it exists
        target = app_data['TARGET'].values if 'TARGET' in app_data.columns else None

        # Basic feature engineering
        app_data = self.engineer_basic_features(app_data)

        # Process additional datasets
        app_data = self._process_bureau_data(app_data, data_paths)
        app_data = self._process_previous_application(app_data, data_paths)
        app_data = self._process_credit_card_balance(app_data, data_paths)
        app_data = self._process_pos_cash_balance(app_data, data_paths)
        app_data = self._process_installments_payments(app_data, data_paths)

        logger.info(f"Final merged shape: {app_data.shape}")
        return app_data, target

    def _process_bureau_data(self, app_data: pd.DataFrame, data_paths: Dict[str, str]) -> pd.DataFrame:
        """Process bureau and bureau balance data."""
        if 'bureau' not in data_paths or not os.path.exists(data_paths['bureau']):
            return app_data

        logger.info("Loading bureau data...")
        bureau = pd.read_csv(data_paths['bureau'])

        # Merge bureau_balance if available
        if 'bureau_balance' in data_paths and os.path.exists(data_paths['bureau_balance']):
            logger.info("Loading bureau_balance data...")
            bureau_balance = pd.read_csv(data_paths['bureau_balance'])

            # Optimized aggregation
            bb_agg = bureau_balance.groupby('SK_ID_BUREAU').agg({
                'MONTHS_BALANCE': ['min', 'max', 'mean'],
                'STATUS': lambda x: (x == '0').sum()
            }).fillna(0)
            bb_agg.columns = ['BB_' + '_'.join(col).strip() for col in bb_agg.columns.values]
            bureau = bureau.merge(bb_agg, on='SK_ID_BUREAU', how='left')

        # Aggregate bureau to application level
        bureau_numeric = bureau.select_dtypes(include=[np.number])
        bureau_agg = bureau_numeric.groupby('SK_ID_CURR').agg({
            col: ['mean', 'max', 'min'] for col in bureau_numeric.columns
            if col not in ['SK_ID_CURR', 'SK_ID_BUREAU']
        }).fillna(0)
        bureau_agg.columns = ['BUREAU_' + '_'.join(col).strip() for col in bureau_agg.columns.values]
        return app_data.merge(bureau_agg, on='SK_ID_CURR', how='left')

    def _process_previous_application(self, app_data: pd.DataFrame, data_paths: Dict[str, str]) -> pd.DataFrame:
        """Process previous application data."""
        if 'previous_application' not in data_paths or not os.path.exists(data_paths['previous_application']):
            return app_data

        logger.info("Loading previous application data...")
        prev_app = pd.read_csv(data_paths['previous_application'])
        prev_numeric = prev_app.select_dtypes(include=[np.number])
        prev_agg = prev_numeric.groupby('SK_ID_CURR').agg({
            col: ['mean', 'max', 'min'] for col in prev_numeric.columns
            if col not in ['SK_ID_CURR', 'SK_ID_PREV']
        }).fillna(0)
        prev_agg.columns = ['PREV_' + '_'.join(col).strip() for col in prev_agg.columns.values]
        return app_data.merge(prev_agg, on='SK_ID_CURR', how='left')

    def _process_credit_card_balance(self, app_data: pd.DataFrame, data_paths: Dict[str, str]) -> pd.DataFrame:
        """Process credit card balance data."""
        if 'credit_card_balance' not in data_paths or not os.path.exists(data_paths['credit_card_balance']):
            return app_data

        logger.info("Loading credit card balance data...")
        cc_balance = pd.read_csv(data_paths['credit_card_balance'])
        cc_numeric = cc_balance.select_dtypes(include=[np.number])
        cc_agg = cc_numeric.groupby('SK_ID_CURR').agg({
            col: ['mean', 'max', 'min'] for col in cc_numeric.columns
            if col not in ['SK_ID_CURR', 'SK_ID_PREV']
        }).fillna(0)
        cc_agg.columns = ['CC_' + '_'.join(col).strip() for col in cc_agg.columns.values]
        return app_data.merge(cc_agg, on='SK_ID_CURR', how='left')

    def _process_pos_cash_balance(self, app_data: pd.DataFrame, data_paths: Dict[str, str]) -> pd.DataFrame:
        """Process POS cash balance data."""
        if 'pos_cash_balance' not in data_paths or not os.path.exists(data_paths['pos_cash_balance']):
            return app_data

        logger.info("Loading POS cash balance data...")
        pos_balance = pd.read_csv(data_paths['pos_cash_balance'])
        pos_numeric = pos_balance.select_dtypes(include=[np.number])
        pos_agg = pos_numeric.groupby('SK_ID_CURR').agg({
            col: ['mean', 'max', 'min'] for col in pos_numeric.columns
            if col not in ['SK_ID_CURR', 'SK_ID_PREV']
        }).fillna(0)
        pos_agg.columns = ['POS_' + '_'.join(col).strip() for col in pos_agg.columns.values]
        return app_data.merge(pos_agg, on='SK_ID_CURR', how='left')

    def _process_installments_payments(self, app_data: pd.DataFrame, data_paths: Dict[str, str]) -> pd.DataFrame:
        """Process installments payments data."""
        if 'installments_payments' not in data_paths or not os.path.exists(data_paths['installments_payments']):
            return app_data

        logger.info("Loading installments payments data...")
        installments = pd.read_csv(data_paths['installments_payments'])

        # Calculate payment difference and ratio
        installments['PAYMENT_DIFF'] = installments['AMT_PAYMENT'] - installments['AMT_INSTALMENT']
        installments['PAYMENT_RATIO'] = installments['AMT_PAYMENT'] / (installments['AMT_INSTALMENT'] + 1e-6)

        inst_numeric = installments.select_dtypes(include=[np.number])
        inst_agg = inst_numeric.groupby('SK_ID_CURR').agg({
            col: ['mean', 'max', 'min'] for col in inst_numeric.columns
            if col not in ['SK_ID_CURR', 'SK_ID_PREV']
        }).fillna(0)
        inst_agg.columns = ['INST_' + '_'.join(col).strip() for col in inst_agg.columns.values]
        return app_data.merge(inst_agg, on='SK_ID_CURR', how='left')

    def engineer_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create basic engineered features with optimized calculations."""
        df = df.copy()  # Avoid modifying original dataframe

        # Credit ratios with safe division
        if 'AMT_CREDIT' in df.columns and 'AMT_INCOME_TOTAL' in df.columns:
            df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / (df['AMT_INCOME_TOTAL'] + 1)

        if 'AMT_ANNUITY' in df.columns and 'AMT_INCOME_TOTAL' in df.columns:
            df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / (df['AMT_INCOME_TOTAL'] + 1)

        if 'AMT_CREDIT' in df.columns and 'AMT_GOODS_PRICE' in df.columns:
            df['CREDIT_GOODS_RATIO'] = df['AMT_CREDIT'] / (df['AMT_GOODS_PRICE'] + 1)

        # Age and employment calculations
        if 'DAYS_BIRTH' in df.columns:
            df['AGE_YEARS'] = -df['DAYS_BIRTH'] / 365.25

        if 'DAYS_EMPLOYED' in df.columns:
            df['EMPLOYMENT_YEARS'] = (-df['DAYS_EMPLOYED'] / 365.25).clip(lower=0)

        if 'DAYS_EMPLOYED' in df.columns and 'DAYS_BIRTH' in df.columns:
            df['DAYS_EMPLOYED_PERCENT'] = df['DAYS_EMPLOYED'] / (df['DAYS_BIRTH'] + 1)

        # External sources aggregation
        ext_source_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']
        ext_source_cols = [col for col in ext_source_cols if col in df.columns]
        if ext_source_cols:
            df['EXT_SOURCE_MEAN'] = df[ext_source_cols].mean(axis=1)
            df['EXT_SOURCE_STD'] = df[ext_source_cols].std(axis=1)
            df['EXT_SOURCE_MIN'] = df[ext_source_cols].min(axis=1)
            df['EXT_SOURCE_MAX'] = df[ext_source_cols].max(axis=1)

        return df

    def apply_windowizing(self, df: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:
        """Apply power transformation (Yeo-Johnson) to skewed numerical features."""
        logger.info("Applying windowizing (Yeo-Johnson transformation) to skewed features...")

        # Target columns for windowizing
        target_cols = ['AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_GOODS_PRICE',
                      'AMT_ANNUITY', 'DAYS_EMPLOYED', 'CREDIT_INCOME_RATIO',
                      'ANNUITY_INCOME_RATIO', 'CREDIT_GOODS_RATIO']

        # Add all AMT_ columns
        amt_cols = [col for col in df.columns if 'AMT_' in col]
        target_cols.extend(amt_cols)
        target_cols = list(set(target_cols))  # Remove duplicates

        transformed_cols = []
        df = df.copy()  # Avoid modifying original dataframe

        for col in target_cols:
            if col in df.columns and df[col].dtype in [np.float64, np.int64]:
                # Check skewness
                skewness = df[col].skew()

                # Transform if highly skewed
                if abs(skewness) > threshold:
                    try:
                        # Use Yeo-Johnson transformation
                        pt = PowerTransformer(method='yeo-johnson', standardize=False)
                        df[col] = pt.fit_transform(df[[col]].values.reshape(-1, 1)).flatten()
                        self.power_transformers[col] = pt  # Save for test data
                        transformed_cols.append(col)
                    except Exception as e:
                        logger.warning(f"Failed to transform {col}: {e}")
                        continue

        logger.info(f"Transformed {len(transformed_cols)} skewed features")
        if transformed_cols and len(transformed_cols) <= 15:
            logger.info(f"Features: {', '.join(transformed_cols[:15])}")

        return df

    def encode_categorical(self, df: pd.DataFrame) -> pd.DataFrame:
        """Encode categorical variables with optimized approach."""
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        logger.info(f"Encoding {len(categorical_cols)} categorical columns...")

        df = df.copy()  # Avoid modifying original dataframe

        for col in categorical_cols:
            if col in ['SK_ID_CURR', 'SK_ID_PREV']:
                continue

            if df[col].nunique() <= 2:
                # Binary encoding
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].fillna('Missing'))
                self.label_encoders[col] = le
            else:
                # One-hot encoding for top 5 categories
                top_cats = df[col].value_counts().head(5).index.tolist()
                for cat in top_cats:
                    df[f"{col}_{cat}"] = (df[col] == cat).astype(int)
                df = df.drop(col, axis=1)

        return df

    def separate_features(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """Separate alternative and traditional features."""
        alternative_keywords = ['FLAG_', 'EXT_SOURCE', 'REGION_', 'OBS_', 'DEF_',
                              'EMAIL', 'PHONE', 'MOBIL', 'SOCIAL']
        traditional_keywords = ['AMT_', 'DAYS_', 'CNT_', 'CREDIT', 'INCOME',
                               'BUREAU_', 'PREV_', 'ANNUITY']

        for col in df.columns:
            if col in ['TARGET', 'SK_ID_CURR']:
                continue

            is_alternative = any(keyword in col.upper() for keyword in alternative_keywords)
            is_traditional = any(keyword in col.upper() for keyword in traditional_keywords)

            if is_alternative and not is_traditional:
                self.alternative_features.append(col)
            else:
                self.traditional_features.append(col)

        self.all_features = df.columns.tolist()
        return self.alternative_features, self.traditional_features

    def preprocess_and_save(self, train_paths: Dict[str, str], test_paths: Optional[Dict[str, str]] = None) -> Dict:
        """Main preprocessing function with windowizing and optimized memory usage."""
        logger.info("="*60)
        logger.info("STEP 1: DATA PREPROCESSING")
        logger.info("="*60)

        # Process training data
        logger.info("Processing training data...")
        train_data, train_target = self.load_and_merge_data(train_paths)

        # Clean data
        logger.info("Cleaning data...")
        train_data = train_data.replace([np.inf, -np.inf], np.nan).fillna(0)

        # Apply windowizing - IMPORTANT: Do this BEFORE encoding
        logger.info("WINDOWIZING - Power transformation for skewed features...")
        train_data = self.apply_windowizing(train_data)

        # Encode categorical
        logger.info("Encoding categorical features...")
        train_data = self.encode_categorical(train_data)

        # Separate features
        logger.info("Separating feature types...")
        alt_features, trad_features = self.separate_features(train_data)
        logger.info(f"Alternative features: {len(alt_features)}")
        logger.info(f"Traditional features: {len(trad_features)}")

        # Prepare for modeling
        X = train_data.drop(['SK_ID_CURR', 'TARGET'], axis=1, errors='ignore')
        y = train_target

        # Train-validation split
        logger.info("Splitting train/validation...")
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state, stratify=y
        )

        # Apply SMOTE with reduced ratio
        logger.info("Applying SMOTE (50% ratio to save memory)...")
        if not HAS_IMBLEARN:
            logger.warning("SMOTE not available, skipping balancing")
            datasets = self._create_datasets_without_smote(X_train, X_val, y_train, y_val, alt_features, trad_features)
        else:
            datasets = self._create_datasets_with_smote(X_train, X_val, y_train, y_val, alt_features, trad_features)

        # Save preprocessed data
        logger.info("Saving preprocessed data...")
        with open('preprocessed_data.pkl', 'wb') as f:
            pickle.dump(datasets, f)

        # Save preprocessor
        with open('preprocessor.pkl', 'wb') as f:
            pickle.dump(self, f)

        logger.info("âœ… Preprocessing complete! Saved to 'preprocessed_data.pkl'")
        logger.info("Dataset sizes:")
        for name, data in datasets.items():
            logger.info(f"- {name}: Train {data['X_train'].shape}, Val {data['X_val'].shape}")

        # Clear memory
        del train_data, X, y, X_train, X_val
        gc.collect()

        return datasets

    def _create_datasets_with_smote(self, X_train, X_val, y_train, y_val, alt_features, trad_features):
        """Create datasets with SMOTE balancing."""
        smote = SMOTE(random_state=self.random_state, sampling_strategy=0.5)
        datasets = {}

        # All features
        scaler_all = StandardScaler()
        X_train_scaled = scaler_all.fit_transform(X_train)
        X_val_scaled = scaler_all.transform(X_val)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train.astype(int))

        datasets['all'] = {
            'X_train': X_train_balanced,
            'X_val': X_val_scaled,
            'y_train': y_train_balanced,
            'y_val': y_val,
            'features': X_train.columns.tolist(),
            'scaler': scaler_all
        }

        # Traditional features
        if trad_features:
            trad_cols = [col for col in X_train.columns if col in trad_features]
            if trad_cols:
                datasets['traditional'] = self._create_feature_dataset(
                    X_train[trad_cols], X_val[trad_cols], y_train, y_val, trad_cols, smote
                )

        # Alternative features
        if alt_features:
            alt_cols = [col for col in X_train.columns if col in alt_features]
            if alt_cols:
                datasets['alternative'] = self._create_feature_dataset(
                    X_train[alt_cols], X_val[alt_cols], y_train, y_val, alt_cols, smote
                )

        return datasets

    def _create_datasets_without_smote(self, X_train, X_val, y_train, y_val, alt_features, trad_features):
        """Create datasets without SMOTE balancing."""
        datasets = {}

        # All features
        scaler_all = StandardScaler()
        X_train_scaled = scaler_all.fit_transform(X_train)
        X_val_scaled = scaler_all.transform(X_val)

        datasets['all'] = {
            'X_train': X_train_scaled,
            'X_val': X_val_scaled,
            'y_train': y_train,
            'y_val': y_val,
            'features': X_train.columns.tolist(),
            'scaler': scaler_all
        }

        # Traditional features
        if trad_features:
            trad_cols = [col for col in X_train.columns if col in trad_features]
            if trad_cols:
                datasets['traditional'] = self._create_feature_dataset_no_smote(
                    X_train[trad_cols], X_val[trad_cols], y_train, y_val, trad_cols
                )

        # Alternative features
        if alt_features:
            alt_cols = [col for col in X_train.columns if col in alt_features]
            if alt_cols:
                datasets['alternative'] = self._create_feature_dataset_no_smote(
                    X_train[alt_cols], X_val[alt_cols], y_train, y_val, alt_cols
                )

        return datasets

    def _create_feature_dataset(self, X_train, X_val, y_train, y_val, feature_cols, smote):
        """Create a dataset for specific features with SMOTE."""
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train.astype(int))

        return {
            'X_train': X_train_balanced,
            'X_val': X_val_scaled,
            'y_train': y_train_balanced,
            'y_val': y_val,
            'features': feature_cols,
            'scaler': scaler
        }

    def _create_feature_dataset_no_smote(self, X_train, X_val, y_train, y_val, feature_cols):
        """Create a dataset for specific features without SMOTE."""
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)

        return {
            'X_train': X_train_scaled,
            'X_val': X_val_scaled,
            'y_train': y_train,
            'y_val': y_val,
            'features': feature_cols,
            'scaler': scaler
        }

# =============================================================================
# Utility Functions
# =============================================================================

def ensure_dir(path: Union[str, Path]) -> None:
    """Create directory if it does not exist."""
    Path(path).mkdir(parents=True, exist_ok=True)


def savefig(path: Union[str, Path], bbox_inches: str = "tight") -> None:
    """Save current matplotlib figure to path and close it."""
    plt.tight_layout()
    plt.savefig(path, dpi=150, bbox_inches=bbox_inches)
    plt.close()


def safe_sample(df: pd.DataFrame, n: int = 100_000, random_state: int = 42) -> pd.DataFrame:
    """Sample up to n rows for plotting to avoid OOM or very slow rendering."""
    if len(df) > n:
        return df.sample(n, random_state=random_state)
    return df


def split_num_cat(df: pd.DataFrame,
                  ignore_cols: Tuple[str, ...] = ("SK_ID_CURR", "SK_ID_PREV", "TARGET")
                  ) -> Tuple[List[str], List[str]]:
    """Split DataFrame columns into numeric and categorical lists."""
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()
    numeric_cols = [c for c in numeric_cols if c not in ignore_cols]
    categorical_cols = [c for c in categorical_cols if c not in ignore_cols]
    return numeric_cols, categorical_cols


def cramers_v(x: pd.Series, y: pd.Series) -> float:
    """Compute CramÃ©r's V between a categorical feature and a binary target."""
    confusion = pd.crosstab(x, y)
    chi2, _, _, _ = chi2_contingency(confusion)
    n = confusion.sum().sum()
    phi2 = chi2 / max(n, 1)
    r, k = confusion.shape
    # Bias correction (Bergsma 2013)
    phi2corr = max(0, phi2 - (k - 1) * (r - 1) / max(n - 1, 1))
    rcorr = r - (r - 1) ** 2 / max(n - 1, 1)
    kcorr = k - (k - 1) ** 2 / max(n - 1, 1)
    denom = max((kcorr - 1), (rcorr - 1), 1e-9)
    return float(np.sqrt(phi2corr / denom))


def compute_vif(df_num: pd.DataFrame) -> pd.DataFrame:
    """Compute Variance Inflation Factors (VIF) for multicollinearity diagnostics."""
    if not HAS_SM or df_num.shape[1] < 2:
        return pd.DataFrame(columns=["feature", "VIF"], data=[])

    X = df_num.copy().replace([np.inf, -np.inf], np.nan).fillna(0)
    X = sm.add_constant(X)
    vifs = []
    for i in range(1, X.shape[1]):  # skip the constant column at 0
        vifs.append(variance_inflation_factor(X.values, i))
    return pd.DataFrame({"feature": df_num.columns, "VIF": vifs}).sort_values("VIF", ascending=False)


def plot_kde_by_target(df: pd.DataFrame, col: str, target_col: str, save_dir: Union[str, Path]) -> None:
    """Plot KDE (or histogram fallback) of a numeric feature split by target."""
    data = df[[col, target_col]].dropna()
    data = safe_sample(data, 100_000)
    plt.figure(figsize=(6, 4))
    try:
        if HAS_SEABORN:
            sns.kdeplot(data=data[data[target_col] == 0], x=col, label="TARGET=0", linewidth=1.5)
            sns.kdeplot(data=data[data[target_col] == 1], x=col, label="TARGET=1", linewidth=1.5)
        else:
            # Fallback: overlaid histograms
            plt.hist(data[data[target_col] == 0][col], bins=40, alpha=0.6, label="TARGET=0", density=True)
            plt.hist(data[data[target_col] == 1][col], bins=40, alpha=0.6, label="TARGET=1", density=True)
        plt.title(f"[KDE] {col} by {target_col}")
        plt.legend()
    except Exception:
        # Absolute fallback: simple hist
        plt.hist([data[data[target_col] == 0][col], data[data[target_col] == 1][col]],
                 bins=40, alpha=0.7, label=["0", "1"])
        plt.title(f"[HIST] {col} by {target_col}")
        plt.legend()

    savefig(os.path.join(save_dir, f"kde_{col}.png"))


def plot_cat_rate(df: pd.DataFrame, col: str, target_col: str, save_dir: Union[str, Path], top_k: int = 10) -> None:
    """Plot top-K categories by count with overlaid bad-rate line."""
    tmp = (df[[col, target_col]]
           .fillna("Missing")
           .groupby(col)
           .agg(count=(target_col, "size"), bad_rate=(target_col, "mean"))
           .sort_values("count", ascending=False))
    tmp_top = tmp.head(top_k)

    fig, ax1 = plt.subplots(figsize=(7, 4))
    ax2 = ax1.twinx()
    ax1.bar(tmp_top.index.astype(str), tmp_top["count"], alpha=0.6)
    ax2.plot(tmp_top.index.astype(str), tmp_top["bad_rate"], marker="o")
    ax1.set_ylabel("Count")
    ax2.set_ylabel("Bad Rate (mean TARGET)")
    ax1.set_title(f"[Categorical] {col}: top-{top_k} count & bad rate")
    plt.xticks(rotation=30, ha="right")
    savefig(os.path.join(save_dir, f"cat_{col}.png"))


def plot_missing_bar(df: pd.DataFrame, save_dir: Union[str, Path], top_k: int = 40) -> None:
    """Plot top-K variables with the highest missing ratio."""
    miss = df.isna().mean().sort_values(ascending=False)
    miss_top = miss.head(top_k)
    plt.figure(figsize=(8, 5))
    if HAS_SEABORN:
        sns.barplot(x=miss_top.values, y=miss_top.index, orient="h")
    else:
        plt.barh(miss_top.index, miss_top.values)
    plt.title("Missing rate (top features)")
    plt.xlabel("Missing ratio")
    savefig(os.path.join(save_dir, "missing_top.png"))


def plot_corr_heatmap(df_num: pd.DataFrame, target_col: str, save_dir: Union[str, Path], top_k: int = 40) -> None:
    """Plot a correlation heatmap focusing on variables most correlated with the target."""
    corr = df_num.corr(numeric_only=True)
    if target_col in corr.columns:
        target_corr = corr[target_col].drop(target_col).abs().sort_values(ascending=False)
        keep = target_corr.head(top_k).index.tolist() + [target_col]
        corr = df_num[keep].corr(numeric_only=True)
    plt.figure(figsize=(10, 8))
    if HAS_SEABORN:
        sns.heatmap(corr, cmap="coolwarm", center=0)
    else:
        plt.imshow(corr.values, aspect="auto")
        plt.colorbar()
        plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
        plt.yticks(range(len(corr.index)), corr.index)
    plt.title("Correlation Heatmap (top by |corr with TARGET|)")
    savefig(os.path.join(save_dir, "corr_heatmap.png"))


def pca_scatter(X: np.ndarray, y: np.ndarray, title: str, save_path: Union[str, Path]) -> None:
    """Project features to 2D via PCA and plot a scatter split by class."""
    X = np.nan_to_num(X)
    pca = PCA(n_components=2, random_state=42)
    Z = pca.fit_transform(X)
    plt.figure(figsize=(6, 5))
    plt.scatter(Z[y == 0, 0], Z[y == 0, 1], s=5, alpha=0.4, label="0")
    plt.scatter(Z[y == 1, 0], Z[y == 1, 1], s=5, alpha=0.6, label="1")
    plt.title(f"PCA 2D: {title}")
    plt.legend()
    savefig(save_path)

# =============================================================================
# Rule-based Feature Rationale
# =============================================================================

def explain_feature(col: str) -> str:
    """Return a human-readable rationale for a feature based on naming patterns."""
    c = col.upper()
    # Handcrafted ratios / features
    if c == "CREDIT_INCOME_RATIO":
        return "Leverage proxy: loan amount vs. income. Higher values imply higher repayment burden."
    if c == "ANNUITY_INCOME_RATIO":
        return "Debt service ratio proxy: annualized annuity vs. income; captures affordability."
    if c == "CREDIT_GOODS_RATIO":
        return "Loan-to-price ratio: loan amount vs. goods price; high values can be risk signal."
    if c == "AGE_YEARS":
        return "Age in years. Risk can be non-linear across ages; important control variable."
    if c == "EMPLOYMENT_YEARS":
        return "Tenure in years. Longer employment often implies income stability and lower risk."
    if c == "DAYS_EMPLOYED_PERCENT":
        return "Employment proportion over lifespan; age-adjusted employment stability."
    if any(x in c for x in ["EXT_SOURCE_MEAN", "EXT_SOURCE_STD", "EXT_SOURCE_MIN", "EXT_SOURCE_MAX"]):
        return "Aggregates external credit scores to stabilize noisy single-source signals."

    # Aggregated families
    if c.startswith("BUREAU_"):
        return "Aggregated bureau-level stats summarizing past credit behavior (e.g., balances, delinquencies)."
    if c.startswith("PREV_"):
        return "Aggregates from previous applications; patterns of approvals/denials relate to risk."
    if c.startswith("CC_"):
        return "Credit card usage/repayment aggregates; sharp utilization spikes can be early warnings."
    if c.startswith("POS_"):
        return "POS cash balance usage aggregates; short-term credit reliance captured."
    if c.startswith("INST_"):
        return "Installment payments aggregates (diff/ratio); adherence to schedule directly reflects behavior."
    if c.startswith("BB_"):
        return "Bureau balance monthly status summarized at SK_ID_BUREAU level."

    # Generic keywords
    if "AMT_" in c:
        return "Amount-level feature; combine with ratios/normalization to reduce scale effects."
    if "DAYS_" in c:
        return "Time/lag feature; duration and recency often correlate with risk."
    if "CNT_" in c:
        return "Count-based feature; activity/application volume can relate to risk."
    if "FLAG_" in c:
        return "Binary indicator; simple yet often strong signals for specific conditions."

    return "Domain- or aggregation-based feature summarizing patterns that models may struggle to derive."


def build_explanation_table(df: pd.DataFrame, save_dir: Union[str, Path], max_rows: int = 200) -> pd.DataFrame:
    """Create a CSV explaining the rationale for up to `max_rows` features."""
    cols = [c for c in df.columns if c not in ("TARGET", "SK_ID_CURR", "SK_ID_PREV")]
    rows = [{"feature": c, "why": explain_feature(c)} for c in cols[:max_rows]]
    tab = pd.DataFrame(rows)
    tab.to_csv(os.path.join(save_dir, "feature_explanations.csv"), index=False)
    return tab

# =============================================================================
# EDA Report Orchestrator
# =============================================================================

class EDAReport:
    """Orchestrates end-to-end EDA, visualizations, and rationale generation."""

    def __init__(self, target_col: str = "TARGET", save_dir: Union[str, Path] = "eda_output", random_state: int = 42) -> None:
        """Initialize EDA report generator."""
        self.target_col = target_col
        self.save_dir = Path(save_dir)
        self.random_state = random_state
        ensure_dir(self.save_dir)
        self.report_lines: List[str] = []

    def add_line(self, text: str = "") -> None:
        """Append a line to the in-memory report and echo to console."""
        logger.info(text)
        self.report_lines.append(text)

    def save_report(self) -> None:
        """Write accumulated report lines to a markdown file."""
        path = self.save_dir / "eda_report.md"
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(self.report_lines))
        logger.info(f"Report saved to: {path}")

    def step_a_overview(self, df: pd.DataFrame) -> None:
        """Dataset overview with missingness and target distribution if available."""
        self.add_line("# STEP A. Dataset Overview")
        self.add_line(f"- shape: {df.shape}")
        miss_ratio = float(df.isna().mean().mean())
        self.add_line(f"- average missing ratio: {miss_ratio:.3f}")
        if self.target_col in df.columns:
            cls_cnt = df[self.target_col].value_counts(dropna=False)
            self.add_line(f"- TARGET distribution: {cls_cnt.to_dict()}")
            plt.figure(figsize=(4, 3))
            cls_cnt.plot(kind="bar")
            plt.title("Target distribution")
            savefig(self.save_dir / "target_distribution.png")
        plot_missing_bar(df, self.save_dir, top_k=40)

    def step_b_numeric(self, df: pd.DataFrame, top_k_skew: int = 6) -> None:
        """Numeric diagnostics: skewness, target-wise density/histograms, correlation heatmap, VIF."""
        self.add_line("\n# STEP B. Numeric Features: Distributions, Skewness, Correlation")
        num_cols, _ = split_num_cat(df, ignore_cols=("SK_ID_CURR", "SK_ID_PREV", self.target_col))
        df_num = df[num_cols + ([self.target_col] if self.target_col in df.columns else [])].copy()

        if num_cols:
            plot_corr_heatmap(df_num, self.target_col, self.save_dir, top_k=40)

        # Highest absolute skewness
        skew_vals = df[num_cols].skew(numeric_only=True).abs().sort_values(ascending=False)
        self.add_line(f"- Top |skew|: {skew_vals.head(top_k_skew).to_dict()}")
        for col in skew_vals.head(top_k_skew).index:
            if self.target_col in df.columns:
                plot_kde_by_target(df, col, self.target_col, self.save_dir)
            else:
                plt.figure(figsize=(6, 4))
                if HAS_SEABORN:
                    sns.histplot(df[col].dropna(), bins=40, kde=True)
                else:
                    plt.hist(df[col].dropna(), bins=40)
                plt.title(f"[HIST] {col}")
                savefig(self.save_dir / f"hist_{col}.png")

        # VIF (if statsmodels available)
        if len(num_cols) > 1 and HAS_SM:
            vif_tab = compute_vif(df[num_cols])
            if len(vif_tab):
                vif_tab.to_csv(self.save_dir / "vif.csv", index=False)
                self.add_line("- Saved VIF table: eda_output/vif.csv (multicollinearity check)")

    def step_c_categorical(self, df: pd.DataFrame, top_k_per_col: int = 10) -> None:
        """Categorical diagnostics: frequency and bad-rate plots, CramÃ©r's V against target."""
        self.add_line("\n# STEP C. Categorical Features: Distributions & Relationship to Target")
        _, cat_cols = split_num_cat(df, ignore_cols=("SK_ID_CURR", "SK_ID_PREV", self.target_col))

        for col in cat_cols:
            try:
                plot_cat_rate(df, col, self.target_col, self.save_dir, top_k=top_k_per_col)
            except Exception as e:
                logger.warning(f"Failed to plot categorical feature {col}: {e}")

        # CramÃ©r's V summary
        rows = []
        for col in cat_cols:
            try:
                v = cramers_v(df[col].fillna("Missing"), df[self.target_col])
                rows.append((col, v))
            except Exception as e:
                logger.warning(f"Failed to compute CramÃ©r's V for {col}: {e}")
                continue
        if rows:
            cramers = pd.DataFrame(rows, columns=["feature", "cramers_v"]).sort_values("cramers_v", ascending=False)
            cramers.to_csv(self.save_dir / "cramers_v.csv", index=False)
            self.add_line("- Saved CramÃ©r's V table: eda_output/cramers_v.csv")

    def step_d_windowizing_demo(self, df: pd.DataFrame, preprocessor, top_k_skew: int = 4) -> None:
        """Visualize distributions before/after Yeoâ€“Johnson power transformation for skewed features."""
        self.add_line("\n# STEP D. Windowizing (Yeoâ€“Johnson) Before/After Comparison")
        num_cols, _ = split_num_cat(df, ignore_cols=("SK_ID_CURR", "SK_ID_PREV", self.target_col))
        skew_vals = df[num_cols].skew(numeric_only=True).abs().sort_values(ascending=False)
        target_cols = skew_vals.head(top_k_skew).index.tolist()

        df_before = df[target_cols + ([self.target_col] if self.target_col in df.columns else [])].copy()
        df_after = df_before.copy()
        df_after = preprocessor.apply_windowizing(df_after, threshold=0.5)

        for col in target_cols:
            base = safe_sample(df_before[[col] + ([self.target_col] if self.target_col in df.columns else [])], 80_000)
            aft = safe_sample(df_after[[col] + ([self.target_col] if self.target_col in df.columns else [])], 80_000)

            plt.figure(figsize=(8, 4))
            plt.subplot(1, 2, 1)
            if HAS_SEABORN:
                sns.histplot(base[col].dropna(), bins=40, kde=True)
            else:
                plt.hist(base[col].dropna(), bins=40)
            plt.title(f"{col}\nBefore (skew={base[col].dropna().skew():.2f})")

            plt.subplot(1, 2, 2)
            if HAS_SEABORN:
                sns.histplot(aft[col].dropna(), bins=40, kde=True)
            else:
                plt.hist(aft[col].dropna(), bins=40)
            plt.title(f"{col}\nAfter (skew={aft[col].dropna().skew():.2f})")

            savefig(self.save_dir / f"windowizing_{col}.png")

        self.add_line("- See plots showing skewness reduction and distribution stabilization after windowizing.")

    def step_e_encoding_strategy(self, df: pd.DataFrame, top_k: int = 5) -> None:
        """Document why we use binary label encoding for 2-level features and Top-K one-hot otherwise."""
        self.add_line("\n# STEP E. Categorical Encoding Strategy Justification")
        _, cat_cols = split_num_cat(df, ignore_cols=("SK_ID_CURR", "SK_ID_PREV", self.target_col))
        rows = []
        for col in cat_cols:
            vc = df[col].value_counts(dropna=False)
            coverage_topk = float(vc.head(top_k).sum() / max(1, vc.sum()))
            rows.append((col, int(vc.nunique()), coverage_topk))
        tab = pd.DataFrame(rows, columns=["feature", "nunique", "coverage_topK"]).sort_values("nunique", ascending=False)
        tab.to_csv(self.save_dir / "categorical_cardinality.csv", index=False)
        self.add_line("- Saved categorical cardinality/coverage: eda_output/categorical_cardinality.csv")
        self.add_line(f"- Rule: nunique â‰¤ 2 â†’ LabelEncoder (binary). Else â†’ Top-{top_k} one-hot to reduce sparsity and memory.")
        self.add_line("- The higher the Top-K coverage, the smaller the information loss due to truncation.")

    def step_f_feature_reason_table(self, df: pd.DataFrame) -> None:
        """Create and save a CSV that explains why each feature exists or is useful."""
        self.add_line("\n# STEP F. Feature Rationale Summary")
        tab = build_explanation_table(df, self.save_dir, max_rows=400)
        self.add_line(f"- Saved: eda_output/feature_explanations.csv (total documented: {len(tab)})")

    def step_g_feature_importance(self, df_encoded: pd.DataFrame) -> None:
        """Estimate feature relevance with Mutual Information and RandomForest importance."""
        self.add_line("\n# STEP G. Mutual Information & RandomForest Importance")
        if self.target_col not in df_encoded.columns:
            self.add_line("- TARGET not found; skipping feature importance.")
            return

        X = df_encoded.drop(columns=[self.target_col, "SK_ID_CURR"], errors="ignore")
        y = df_encoded[self.target_col].astype(int)
        X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

        # Mutual Information
        try:
            mi = mutual_info_classif(X, y, discrete_features=False, random_state=self.random_state)
            mi_tab = pd.DataFrame({"feature": X.columns, "MI": mi}).sort_values("MI", ascending=False)
            mi_tab.head(100).to_csv(self.save_dir / "mi_top100.csv", index=False)
            self.add_line("- Saved MI top-100: eda_output/mi_top100.csv")
        except Exception as e:
            self.add_line(f"- MI computation failed: {e}")

        # RandomForest importance + validation AUC
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state, stratify=y
        )
        rf = RandomForestClassifier(
            n_estimators=500, max_depth=None, n_jobs=-1, random_state=self.random_state, class_weight=None
        )
        rf.fit(X_train, y_train)
        pred = rf.predict_proba(X_val)[:, 1]
        auc_val = roc_auc_score(y_val, pred)
        self.add_line(f"- RandomForest AUC (validation): {auc_val:.4f}")

        imp = pd.DataFrame({"feature": X.columns, "importance": rf.feature_importances_})\
            .sort_values("importance", ascending=False)
        imp.head(100).to_csv(self.save_dir / "rf_importance_top100.csv", index=False)

        # Plot top 30 importances
        top = imp.head(30).sort_values("importance")
        plt.figure(figsize=(7, 8))
        plt.barh(top["feature"], top["importance"])
        plt.title("RandomForest Feature Importance (top 30)")
        savefig(self.save_dir / "rf_importance_top30.png")

    def step_h_smote_demo(self, df_encoded: pd.DataFrame, smote_ratio: float = 0.5) -> None:
        """Visualize class imbalance and the effect of SMOTE (train-only demo)."""
        self.add_line("\n# STEP H. Class Imbalance & SMOTE (Visualization Demo)")
        if self.target_col not in df_encoded.columns:
            self.add_line("- TARGET not found; skipping SMOTE demo.")
            return
        if not HAS_IMBLEARN:
            self.add_line("- imbalanced-learn not available; skipping SMOTE demo.")
            return

        X = df_encoded.drop(columns=[self.target_col, "SK_ID_CURR"], errors="ignore")
        y = df_encoded[self.target_col].astype(int)

        # Scale only for visualization (PCA)
        scaler = StandardScaler(with_mean=True, with_std=True)
        Xs = scaler.fit_transform(X.replace([np.inf, -np.inf], np.nan).fillna(0))

        # Split
        X_tr, _, y_tr, _ = train_test_split(
            Xs, y, test_size=0.2, random_state=self.random_state, stratify=y
        )

        # Before SMOTE
        pca_scatter(X_tr, y_tr.values, "Before SMOTE (train)", self.save_dir / "pca_before_smote.png")
        self.add_line(f"- Before SMOTE class counts: {pd.Series(y_tr).value_counts().to_dict()}")

        # After SMOTE
        smote = SMOTE(random_state=self.random_state, sampling_strategy=smote_ratio)
        X_tr_sm, y_tr_sm = smote.fit_resample(X_tr, y_tr)
        pca_scatter(X_tr_sm, y_tr_sm.values, f"After SMOTE (sampling={smote_ratio})",
                    self.save_dir / "pca_after_smote.png")
        self.add_line(f"- After SMOTE class counts: {pd.Series(y_tr_sm).value_counts().to_dict()}")
        self.add_line("- Note: Apply SMOTE only on the training fold to prevent leakage.")

    def run_all(self, merged_df: pd.DataFrame, preprocessor) -> None:
        """Execute all EDA steps on the merged (engineered) DataFrame."""
        logger.info("ðŸš€ Starting comprehensive EDA analysis...")

        # Define EDA steps for progress tracking
        total_steps = 9
        progress = ProgressTracker(total_steps, "EDA Analysis Pipeline")

        # Aâ€“C: Work on the raw engineered data (encoding not applied yet)
        progress.start_step("Dataset overview and missing value analysis")
        self.step_a_overview(merged_df)
        progress.end_step()

        progress.start_step("Numeric feature analysis (distributions, skewness, correlations)")
        self.step_b_numeric(merged_df, top_k_skew=6)
        progress.end_step()

        progress.start_step("Categorical feature analysis with target relationships")
        self.step_c_categorical(merged_df, top_k_per_col=10)
        progress.end_step()

        # D: Windowizing (power transform) demonstration
        progress.start_step("Windowizing demonstration (before/after transformations)")
        self.step_d_windowizing_demo(merged_df.copy(), preprocessor, top_k_skew=4)
        progress.end_step()

        # E: Encoding strategy justification
        progress.start_step("Categorical encoding strategy justification")
        self.step_e_encoding_strategy(merged_df)
        progress.end_step()

        # F: Feature rationale table (rule-based)
        progress.start_step("Feature rationale documentation")
        self.step_f_feature_reason_table(merged_df)
        progress.end_step()

        # Encode (using the same strategy as your preprocessor) for Gâ€“H analysis
        progress.start_step("Encoding data for downstream analysis")
        self.add_line("\n# Encoding (for downstream analysis; mirrors your production strategy)")
        encoded_df = preprocessor.encode_categorical(merged_df.copy())
        encoded_df = encoded_df.replace([np.inf, -np.inf], np.nan).fillna(0)
        progress.end_step()

        # G: MI + RandomForest importance
        progress.start_step("Feature importance analysis (MI + Random Forest)")
        self.step_g_feature_importance(encoded_df)
        progress.end_step()

        # H: SMOTE visualization demo (train only)
        progress.start_step("SMOTE visualization and class imbalance analysis")
        self.step_h_smote_demo(encoded_df, smote_ratio=0.5)
        progress.end_step()

        # Save markdown report
        progress.start_step("Saving EDA report")
        self.save_report()
        progress.end_step()

        progress.finish()
        logger.info("ðŸŽ‰ EDA analysis completed successfully!")

# =============================================================================
# Run the full preprocessing pipeline and save arrays
# =============================================================================

def run_preprocessing_and_save(train_paths: Dict[str, str]) -> Tuple[DataPreprocessor, Dict]:
    """Run the full preprocessing pipeline using the provided DataPreprocessor."""
    preproc = DataPreprocessor()
    datasets = preproc.preprocess_and_save(train_paths)
    return preproc, datasets

# =============================================================================
# Example entry point
# =============================================================================
if __name__ == "__main__":
    print("="*80)
    print("ðŸŽ¯ EDA + FEATURE ENGINEERING PIPELINE")
    print("="*80)
    print("ðŸ“‹ This pipeline will:")
    print("   1. Load and merge multiple datasets")
    print("   2. Perform comprehensive EDA analysis")
    print("   3. Generate preprocessed datasets for modeling")
    print("="*80)

    print("ðŸš€ Initializing pipeline...")

    # 1) File paths: Home Credit style
    train_paths = {
        "application": "application_train.csv",        # required
        "bureau": "bureau.csv",
        "bureau_balance": "bureau_balance.csv",
        "previous_application": "previous_application.csv",
        "credit_card_balance": "credit_card_balance.csv",
        "pos_cash_balance": "POS_CASH_balance.csv",
        "installments_payments": "installments_payments.csv",
    }

    print("ðŸ“‚ Configured data paths:")
    for name, path in train_paths.items():
        status = "âœ…" if os.path.exists(path) else "âŒ"
        print(f"   {status} {name}: {path}")

    # Track overall pipeline progress
    pipeline_progress = ProgressTracker(3, "Complete Pipeline")

    # 2) Load & merge using provided DataPreprocessor
    pipeline_progress.start_step("Loading and merging datasets")
    print("ðŸ”§ Initializing DataPreprocessor...")
    preproc = DataPreprocessor()
    merged_df, target = preproc.load_and_merge_data(train_paths)
    pipeline_progress.end_step()

    # 3) Run EDA report (explains the WHY behind windowizing, encoding, SMOTE, and feature creation)
    pipeline_progress.start_step("Running comprehensive EDA analysis")
    print("ðŸ“Š Initializing EDA Report generator...")
    eda = EDAReport(target_col="TARGET", save_dir="eda_output", random_state=42)
    eda.run_all(merged_df, preprocessor=preproc)
    pipeline_progress.end_step()

    # 4) Actually produce train-ready arrays and persist them
    pipeline_progress.start_step("Generating preprocessed datasets")
    print("ðŸ”„ Running final preprocessing pipeline...")
    preproc2, datasets = run_preprocessing_and_save(train_paths)
    pipeline_progress.end_step()

    pipeline_progress.finish()
    print("="*80)
    print("ðŸŽ‰ PIPELINE COMPLETED SUCCESSFULLY!")
    print("="*80)
    print("ðŸ“ Output files:")
    print("   ðŸ“Š EDA Report: eda_output/eda_report.md")
    print("   ðŸ–¼ï¸  Visualizations: eda_output/*.png")
    print("   ðŸ“ˆ Feature Analysis: eda_output/*.csv")
    print("   ðŸ’¾ Preprocessed Data: preprocessed_data.pkl")
    print("   ðŸ”§ Preprocessor: preprocessor.pkl")
    print("="*80)

    gc.collect()

