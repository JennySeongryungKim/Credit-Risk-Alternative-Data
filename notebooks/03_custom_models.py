# -*- coding: utf-8 -*-
"""03_Custom_Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sPoHrtn-1oFJbTnlrsZ7VIk1l8R9wo3N
"""

import numpy as np
from typing import Optional
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.linear_model import LinearRegression
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

class TabNetClassifier(BaseEstimator, ClassifierMixin):
    """Simplified TabNet implementation for binary classification"""

    def __init__(self, n_steps=3, feature_dim=8, output_dim=8,
                 learning_rate=0.02, batch_size=256, epochs=50):
        self.n_steps = n_steps
        self.feature_dim = feature_dim
        self.output_dim = output_dim
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.epochs = epochs
        self.model = None
        self.classes_ = np.array([0, 1])

    def _build_model(self, input_dim):
        """Build TabNet architecture"""
        inputs = Input(shape=(input_dim,))

        # Feature transformation
        x = layers.Dense(self.feature_dim * 2, activation='relu')(inputs)
        x = layers.BatchNormalization()(x)

        # Attention mechanism (simplified)
        for step in range(self.n_steps):
            # Split features
            attended = layers.Dense(self.feature_dim, activation='relu')(x)
            attended = layers.BatchNormalization()(attended)

            # Feature selection
            mask = layers.Dense(input_dim, activation='sigmoid')(attended)
            masked_features = layers.Multiply()([inputs, mask])

            # Transform selected features
            x = layers.Dense(self.feature_dim, activation='relu')(masked_features)
            x = layers.BatchNormalization()(x)
            x = layers.Dropout(0.1)(x)

        # Output layer
        output = layers.Dense(1, activation='sigmoid')(x)

        model = Model(inputs=inputs, outputs=output)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        return model

    def fit(self, X, y):
        """Train TabNet model"""
        # Build model
        self.model = self._build_model(X.shape[1])

        # Callbacks
        callbacks = [
            EarlyStopping(patience=5, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=3)
        ]

        # Train
        self.model.fit(
            X, y,
            batch_size=self.batch_size,
            epochs=self.epochs,
            validation_split=0.1,
            callbacks=callbacks,
            verbose=0
        )

        return self

    def predict_proba(self, X):
        """Predict probabilities - FIXED"""
        if len(X) == 0:
            return np.array([])

        try:
            # Ensure X is the right format
            X = np.asarray(X).astype(np.float32)
            pos_proba = self.model.predict(X, verbose=0, batch_size=256).flatten()
            neg_proba = 1 - pos_proba
            return np.column_stack([neg_proba, pos_proba])
        except Exception as e:
            # Fallback to random predictions if model fails
            print(f"TabNet prediction error: {e}")
            return np.column_stack([np.ones(len(X))*0.5, np.ones(len(X))*0.5])

    def predict(self, X):
        """Predict classes"""
        return (self.predict_proba(X)[:, 1] > 0.5).astype(int)

    def get_params(self, deep=True):
        """Get parameters for sklearn compatibility"""
        return {
            'n_steps': self.n_steps,
            'feature_dim': self.feature_dim,
            'output_dim': self.output_dim,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size,
            'epochs': self.epochs
        }

    def set_params(self, **params):
        """Set parameters for sklearn compatibility"""
        for param, value in params.items():
            setattr(self, param, value)
        return self




class WideDeepClassifier(BaseEstimator, ClassifierMixin):
    """Wide & Deep Network for binary classification"""

    def __init__(self, deep_layers=[128, 64, 32], learning_rate=0.001,
                 batch_size=256, epochs=50):
        self.deep_layers = deep_layers
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.epochs = epochs
        self.model = None
        self.classes_ = np.array([0, 1])

    def _build_model(self, input_dim):
        """Build Wide & Deep architecture"""
        inputs = Input(shape=(input_dim,))

        # Wide part (linear model)
        wide = layers.Dense(1, activation='linear')(inputs)

        # Deep part (DNN)
        deep = inputs
        for units in self.deep_layers:
            deep = layers.Dense(units, activation='relu')(deep)
            deep = layers.BatchNormalization()(deep)
            deep = layers.Dropout(0.2)(deep)

        deep_output = layers.Dense(1, activation='linear')(deep)

        # Combine Wide and Deep
        combined = layers.Add()([wide, deep_output])
        output = layers.Activation('sigmoid')(combined)

        model = Model(inputs=inputs, outputs=output)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        return model

    def fit(self, X, y):
        """Train Wide & Deep model"""
        # Build model
        self.model = self._build_model(X.shape[1])

        # Callbacks
        callbacks = [
            EarlyStopping(patience=5, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=3)
        ]

        # Train
        self.model.fit(
            X, y,
            batch_size=self.batch_size,
            epochs=self.epochs,
            validation_split=0.1,
            callbacks=callbacks,
            verbose=0
        )

        return self

    def predict_proba(self, X):
        """Predict probabilities - FIXED"""
        if len(X) == 0:
            return np.array([])

        try:
            # Ensure X is the right format
            X = np.asarray(X).astype(np.float32)
            pos_proba = self.model.predict(X, verbose=0, batch_size=256).flatten()
            neg_proba = 1 - pos_proba
            return np.column_stack([neg_proba, pos_proba])
        except Exception as e:
            # Fallback to random predictions if model fails
            print(f"Wide&Deep prediction error: {e}")
            return np.column_stack([np.ones(len(X))*0.5, np.ones(len(X))*0.5])

    def predict(self, X):
        """Predict classes"""
        return (self.predict_proba(X)[:, 1] > 0.5).astype(int)

    def get_params(self, deep=True):
        """Get parameters for sklearn compatibility"""
        return {
            'deep_layers': self.deep_layers,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size,
            'epochs': self.epochs
        }

    def set_params(self, **params):
        """Set parameters for sklearn compatibility"""
        for param, value in params.items():
            setattr(self, param, value)
        return self


class LinearRegressionClassifier(BaseEstimator, ClassifierMixin):
    """Linear Regression wrapper for binary classification"""

    def __init__(self):
        self.model = LinearRegression()
        self.threshold = 0.5
        self.classes_ = np.array([0, 1])

    def fit(self, X, y):
        """Train linear regression"""
        self.model.fit(X, y)
        # Find optimal threshold
        y_pred_cont = self.model.predict(X)
        thresholds = np.linspace(y_pred_cont.min(), y_pred_cont.max(), 100)
        best_acc = 0
        for t in thresholds:
            acc = ((y_pred_cont > t).astype(int) == y).mean()
            if acc > best_acc:
                best_acc = acc
                self.threshold = t
        return self

    def predict_proba(self, X):
        """Predict probabilities"""
        y_pred = self.model.predict(X)
        # Clip to [0, 1] range
        y_pred = np.clip(y_pred, 0, 1)
        return np.column_stack([1 - y_pred, y_pred])

    def predict(self, X):
        """Predict classes"""
        y_pred = self.model.predict(X)
        return (y_pred > self.threshold).astype(int)

    def get_params(self, deep=True):
        """Get parameters"""
        return {}

    def set_params(self, **params):
        """Set parameters"""
        return self